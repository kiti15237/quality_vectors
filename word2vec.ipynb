{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import time\n",
    "import pickle\n",
    "import sets\n",
    "import keras\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "file = open(\"data/silva/freshwater/glove_input_freshwater.txt\")\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    line = line.split(\"\\t\")\n",
    "    documents.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictionary = {}\n",
    "reverse_dictionary = {}\n",
    "unlisted = [i for sublist in documents for i in sublist]\n",
    "freq = Counter(unlisted)\n",
    "vocab_size = len(freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for tup in freq.most_common():\n",
    "    dictionary[tup[0]] = i\n",
    "    reverse_dictionary[i] = tup[0]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for doc in documents:\n",
    "    docs.append([dictionary[taxa] for taxa in doc])\n",
    "np.random.seed(10)\n",
    "choice = np.random.choice(range(len(docs)), int(3*len(docs)/4)) \n",
    "docs_train = np.array(docs)[choice]\n",
    "print(len(docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increment = 10\n",
    "total_docs = 50\n",
    "def subsetDoc(i):\n",
    "    print(\"subsetting document\")\n",
    "    start = 0 + (increment *i)\n",
    "    end = increment +  (increment * i)\n",
    "    docs_small = docs_train[start:end]\n",
    "    return(docs_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCouples(chunk):\n",
    "    import itertools\n",
    "    import pickle\n",
    "    couples = []\n",
    "    labels = []\n",
    "    start = 0 + (increment *chunk)\n",
    "    end = increment +  (increment * chunk)\n",
    "    docs_small = docs_train[start:end]\n",
    "    for doc in docs_small:\n",
    "        pairs = itertools.combinations(doc, r=2)\n",
    "        for i,j in pairs:\n",
    "\n",
    "            couples.append([i,j])\n",
    "            labels.append(1)\n",
    "    couple_file = open('contexts/couples_' + str(chunk) + '.obj', 'wb')\n",
    "    label_file = open('contexts/labels_' + str(chunk) + '.obj', 'wb')\n",
    "    pickle.dump(couples, couple_file)\n",
    "    pickle.dump(labels, label_file)\n",
    "    couple_file.close()\n",
    "    label_file.close()\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview = c[:]\n",
    "dview.push({'docs_train': docs_train})\n",
    "dview.push({'increment': increment})\n",
    "#dview.push({'sampling_table': sampling_table})\n",
    "start_time = time.time()\n",
    "dview.map_sync(getCouples, range(int(50/10)))\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't understand why, but there are a number of samples that are just taxa 18568 and NO other taxa. Not sure why this is\n",
    "#but pretty sure it's not meaningful data\n",
    "docs_train_new = []\n",
    "delete = []\n",
    "for i in range(len(docs_train)):\n",
    "    doc = docs_train[i]\n",
    "    #18568 = number of taxa in full dataset\n",
    "    #6581 = number of taxa in 5perc dataset\n",
    "    if doc == [6581]:\n",
    "        print(i)\n",
    "        delete.append(i)\n",
    "docs_train_new = np.delete(docs_train, delete)\n",
    "print(len(docs_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make hashtable\n",
    "ntaxa = len(dictionary.keys())\n",
    "taxa_dict = []\n",
    "for i in range(ntaxa):\n",
    "    taxa_dict.append(set())\n",
    "\n",
    "sample = 0\n",
    "for doc in docs_train_new:  \n",
    "    for taxa in doc:\n",
    "        taxa_dict[taxa].add(sample)\n",
    "    sample += 1\n",
    "\n",
    "del taxa_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(taxa_dict))\n",
    "#18567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegEx(id_interest):\n",
    "    from random import shuffle\n",
    "    neg_hits = 0\n",
    "    neg_hit_taxa = []\n",
    "    shuffle(ids)\n",
    "    counter = 0\n",
    "    numNegs = 10\n",
    "    while neg_hits < numNegs and counter < len(ids):\n",
    "        numSamplesShared = len(taxa_dict[id_interest].intersection(taxa_dict[ids[counter]]))\n",
    "        if numSamplesShared == 0:\n",
    "            neg_hits += 1\n",
    "            neg_hit_taxa.append(ids[counter])\n",
    "        counter += 1\n",
    "    return([id_interest, neg_hit_taxa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = p.Client()\n",
    "dview = c[:]\n",
    "\n",
    "ntaxa = len(taxa_dict)\n",
    "ids = [i for i in range(ntaxa)]\n",
    "dview.push({'taxa_dict': taxa_dict})\n",
    "dview.push({'ids': ids})\n",
    "neg_hits = dview.map_sync(getNegEx, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples_neg = []\n",
    "labels_neg = []\n",
    "for block in neg_hits:\n",
    "    for neg_partner in block[1]:\n",
    "        couples_neg.append([block[0], neg_partner])\n",
    "        labels_neg.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "neg_couples = []\n",
    "labels = []\n",
    "for taxa_id in range(ntaxa):\n",
    "    neg_ex = getNegEx(taxa_id, numNegs = 2000)\n",
    "    for neg_taxa_id in neg_ex:\n",
    "        neg_couples.append([taxa_id, neg_taxa_id])\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_couples_file = open('neg_couples.obj', 'wb')\n",
    "neg_labels_file = open('neg_labels.obj', 'wb')\n",
    "\n",
    "pickle.dump(neg_couples, neg_couples_file)\n",
    "pickle.dump(labels, neg_labels_file)\n",
    "neg_couples_file.close()\n",
    "neg_labels_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2bc9f6874095>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0minput_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"embedding\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Reshape, Dot\n",
    "from keras.models import Model \n",
    "vector_dim = 500\n",
    "\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name = \"embedding\")\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "similarity = Dot(axes = 0, normalize = True)([target, context]) # cosine similarity (end up with scalar)\n",
    "\n",
    "dot_product = Dot(axes = 1, normalize = False)([target, context]) #Component-wise multiply\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[input_target, input_context], outputs = output)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = 'rmsprop')\n",
    "\n",
    "\n",
    "validation_model = Model(inputs=[input_target, input_context], outputs = similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        for i in range(vocab_size):\n",
    "            in_arr1[0,] = valid_word_idx\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples_pos_file = open(\"contexts/pos/couples_0.obj\", \"rb\")\n",
    "couples_pos = pickle.load(couples_pos_file)\n",
    "\n",
    "labels_pos_file = open(\"contexts/pos/labels_0.obj\", \"rb\")\n",
    "labels_pos = pickle.load(labels_pos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples_neg = []\n",
    "labels_neg = []\n",
    "for i in range(100):\n",
    "    couples_neg_file = open(\"contexts/neg/couples_\" + str(i) + \".obj\", \"rb\")\n",
    "    tmp = pickle.load(couples_neg_file)\n",
    "    couples_neg = couples_neg + tmp\n",
    "    couples_neg_file.close()\n",
    "\n",
    "    labels_neg_file = open(\"contexts/neg/labels_\" + str(i) + \".obj\", \"rb\")\n",
    "    tmp = pickle.load(labels_neg_file)\n",
    "    labels_neg = labels_neg + tmp\n",
    "    labels_neg_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(couples_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples_use = couples_pos + couples_neg\n",
    "labels_use = labels_pos + labels_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_target, word_context = zip(*couples_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels_use[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    #if cnt % 10000 == 0:\n",
    "     #   sim_cb.run_sim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
